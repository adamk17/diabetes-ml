# ðŸ©º diabetes-ml

A complete machine learning pipeline for predicting diabetes progression using FastAPI, TensorFlow (Keras), PostgreSQL, Docker, AWS (S3 + RDS), and Terraform.

This project showcases how to train a regression model, serve it as an API, and deploy it with cloud infrastructure â€“ all production-ready and containerized.

---

## ðŸš€ Features

- âœ… **ML model** trained with TensorFlow (Keras) on diabetes dataset
- âœ… **Scaler saved** for consistent inference
- âœ… **FastAPI REST API** for real-time predictions
- âœ… **S3 model loading** in containerized app
- âœ… **PostgreSQL logging** of predictions for audit/monitoring
- âœ… **Prometheus metrics** via /metrics
- âœ… **Swagger UI** for interactive API docs
- âœ… **Docker + Docker Compose** support for local development and testing
- âœ… **Terraform infrastructure** (EC2, RDS, IAM, S3) for AWS

---

### ðŸ§  ML Training Pipeline

Trained regression model that uses TensorFlow (Keras) on the classic diabetes dataset (`442 samples`, `10 features`).

#### ðŸ”§ Techniques Used

- **Model architecture**: 2 hidden layers with ReLU, batch normalization, dropout, and L2 regularization
- **Callbacks**:
  - Early stopping (patience = 30)
  - ReduceLROnPlateau for dynamic learning rate adjustment
- **Data**: StandardScaler applied to features
- **Artifacts saved**:
  - `tf_model.h5` â€“ trained model
  - `scaler.pkl` â€“ fitted scaler
  - Visualizations: feature importance, training history, prediction scatter

---

### âš™ï¸ API Overview

The FastAPI server exposes the following endpoints:

- `POST /predict`  
  Takes 10 float features as input, returns prediction, timestamp, and unique request ID.

- `GET /health`  
  Checks whether model, scaler, and database connection are ready.

- `GET /docs`  
  Built-in interactive documentation via Swagger UI.

- `GET /metrics`  
  Exposes Prometheus-compatible metrics (latency, request count, etc).

---

### ðŸ³ Local Development with Docker Compose

To run the API and PostgreSQL locally using Docker:

1. **Make sure** you have Docker and Docker Compose installed.
2. **Create** a `.env.local` file inside the `api/` directory with all required environment variables.
3. **Run** the following command from the project root:

```bash
docker-compose --env-file api/.env.local up --build
```

#### âœ… What it does:
* Automatically downloads the model (`.h5`) and scaler (`.pkl`) from S3 during startup
* Spins up a local PostgreSQL database container
* Exposes:
   * Prometheus metrics â†’ http://localhost:8000/metrics
   * Swagger UI â†’ http://localhost:8000/docs

To shut everything down:

```bash
docker-compose down
```

### ðŸ“‚ Project Structure (simplified)
* `api/app/` â†’ main FastAPI application (config, routes, services, etc.)
* `api/.env.local` â†’ environment variables for local development (used by Docker)
* `api/Dockerfile` â†’ image definition for the FastAPI app
* `api/upload_to_s3.py` â†’ utility script to upload model artifacts to S3
* `api/train_model.py` â†’ one-time script to train and export the model
* `docker-compose.yml` â†’ local app + DB stack
* `requirements.txt` â†’ Python dependencies for the container
* `trained_model/` â†’ saved model, scaler, and training visualizations

---

### â˜ï¸ AWS Integration

This project uses several AWS services:

- **Amazon S3** â€“ to store trained model and scaler (`.h5`, `.pkl`)
- **Amazon RDS** â€“ to log API predictions with timestamps and metadata
- **IAM** â€“ to separate programmatic access (via environment)
- **Optional (future)**: EC2, CloudWatch, ElastiCache, Secrets Manager

Model and scaler are dynamically loaded from S3 with script.  
All S3 uploads are managed by a dedicated upload script using `boto3`.

Local setup does not require AWS RDS. By default, Docker Compose runs a local PostgreSQL instance.

---

### ðŸ“Š Visualizations & Analysis

#### 1. Feature Importance

Shows which features the model considered most influential based on the absolute weights of the first layer:

![Feature Importance](./trained_model/feature_importance.png)

#### 2. Training & Validation MAE

Model converges nicely without overfitting â€“ early stopping helps retain generalization:

![Training Plot](./trained_model/training_plot.png)

#### 3. Prediction Scatter Plot

Visual comparison of predicted vs actual values on the test set:

![Prediction Scatter](./trained_model/prediction_scatter.png)

- Predictions generally follow the diagonal line (ideal prediction), especially for values below 150.
- There's noticeable spread in higher values (>200), indicating that the model struggles more in that range.
- Outliers show some samples with significant prediction errors.
- This is likely due to the **small dataset size (442 samples)**, which limits generalization capacity.